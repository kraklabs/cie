# CIE Docker Environment Configuration
#
# Copy this file to .env and customize for your environment:
#   cp .env.example .env
#
# All variables have sensible defaults and are optional unless marked as required.

# ============================================================================
# Core CIE Settings
# ============================================================================

# Project identifier (required)
# Used to namespace the index and configuration
CIE_PROJECT_ID=my-project

# Logging level
# Options: debug, info, warn, error
# Default: info
CIE_LOG_LEVEL=info

# ============================================================================
# Embedding Provider (Ollama)
# ============================================================================

# Ollama embedding model
# Default: nomic-embed-text (~1GB download on first use)
# Alternatives: mxbai-embed-large, all-minilm
OLLAMA_EMBED_MODEL=nomic-embed-text

# ============================================================================
# Optional: LLM for Narrative Generation
# ============================================================================

# Enable LLM-powered narrative for the `cie analyze` command
# Uncomment to use Ollama for narrative generation:

# CIE_LLM_URL=http://ollama:11434
# CIE_LLM_MODEL=llama3

# For OpenAI-compatible APIs:
# CIE_LLM_URL=https://api.openai.com/v1
# CIE_LLM_MODEL=gpt-4
# CIE_LLM_API_KEY=your-api-key-here

# ============================================================================
# Advanced: Primary Hub / Edge Cache (Distributed Setup)
# ============================================================================

# For distributed CIE architecture, configure these:
# (See docker-compose.cie.yml in the root directory for full example)

# CIE_PRIMARY_HUB=primary-hub:50051
# CIE_BASE_URL=http://edge-cache:8080
